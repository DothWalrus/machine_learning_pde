{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "vietnamese-credit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "import tikzplotlib\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "little-laugh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section is just me figuring out how tensors work\n",
    "#lower_bound = tf.zeros(shape=[1,1]) # Lower bound u(0)=0\n",
    "#upper_bound = tf.ones(shape=[1,1]) # Upper bound u(1)=1\n",
    "#x = tf.random.uniform(shape=[1,100]) # Interior points\n",
    "#training_pts = tf.concat([lower_bound, x, upper_bound], 1) # Actual training data\n",
    "#x_0 = tf.slice(training_pts, [0,0], [1,1])\n",
    "#x_1 = tf.slice(training_pts, [0,101], [1,1])\n",
    "#print(x_0.numpy(), x_1.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "returning-superintendent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct Simple Feedforward Network\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_shape=[1], activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "#model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "placed-venture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Loss Function\n",
    "def loss_fn(model,x):\n",
    "    #print(x)\n",
    "    with tf.GradientTape() as t:\n",
    "        t.watch(x)\n",
    "        f = model(x, training=True)\n",
    "        #print('f = ', f)\n",
    "    df_dx = t.gradient(f,x)\n",
    "    u_0 = tf.zeros(shape=[1,1]) # Lower boundary condition\n",
    "    u_1 = tf.ones(shape=[1,1]) # Upper boundary condition\n",
    "    bound_weight = 100 # lambda = sqrt(bound_weight)\n",
    "    mse = tf.keras.losses.MeanSquaredError()\n",
    "    lower_bound_error = tf.math.subtract(tf.slice(f,[0,0],[1,1]),u_0)\n",
    "    #print(lower_bound_error)\n",
    "    upper_bound_error = tf.math.subtract(tf.slice(f,[99,0], [1,1]),u_1)\n",
    "    loss_1 = tf.math.add(tf.nn.l2_loss(df_dx), \n",
    "                       tf.nn.l2_loss(bound_weight*lower_bound_error))\n",
    "    loss = tf.math.add(loss_1, tf.nn.l2_loss(bound_weight*upper_bound_error))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "excited-writing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Train network\n",
    "    optimizer = tf.keras.optimizers.Adam() # Fancy gradient decent\n",
    "    epochs = 20\n",
    "    train_loss_results = [] # For tracking loss during training\n",
    "    iterations_per_epoch = 100\n",
    "    minibatch_size = 100 # Number of points to be selected each iteration\n",
    "    average_error = 1\n",
    "    epoch = -1\n",
    "    while average_error > 0.005: \n",
    "    #for epoch in range(epochs): # Uncomment to run for specific number of epochs\n",
    "        epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "        for iteration in range(iterations_per_epoch):\n",
    "            lower_bound = tf.zeros(shape=[1,1]) # Lower bound u(0)=0\n",
    "            upper_bound = tf.ones(shape=[1,1]) # Upper bound u(1)=1\n",
    "            interior_pts = tf.random.uniform(shape=[1,minibatch_size-2]) # Interior points\n",
    "            x = tf.transpose(tf.concat([lower_bound, interior_pts, upper_bound], 1)) # Actual training data\n",
    "            with tf.GradientTape() as t:\n",
    "                t.watch(x)\n",
    "                #print(x)\n",
    "                #f = model(x, training=True) # Estimate for u\n",
    "                loss = loss_fn(model,x) # Loss\n",
    "            grads = t.gradient(loss, model.trainable_weights) # Find model gradients\n",
    "            optimizer.apply_gradients(zip(grads,model.trainable_weights)) # Perform gradient decent\n",
    "            epoch_loss_avg.update_state(loss) # Track loss\n",
    "            # End training iteration\n",
    "        # Check average error    \n",
    "        lower_bound = tf.zeros(shape=[1,1]) # Lower bound u(0)=0\n",
    "        upper_bound = tf.ones(shape=[1,1]) # Upper bound u(1)=1\n",
    "        x = tf.sort(tf.random.uniform(shape=[1,98])) # Interior points\n",
    "        test_points_tensor = tf.transpose(tf.concat([lower_bound, x, upper_bound], 1)) # Actual data\n",
    "        test_points = test_points_tensor.numpy()\n",
    "        g = model(test_points_tensor, training=False).numpy()\n",
    "        average_error = np.sum(abs(g-test_points))/len(g)\n",
    "        print(average_error)\n",
    "        \n",
    "        train_loss_results.append(epoch_loss_avg.result())\n",
    "        epoch+=1 \n",
    "        print(\"Epoch {:03d}: Loss: {:.3f}\".format(epoch, epoch_loss_avg.result()))\n",
    "        # End Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "crucial-mortality",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01731978416442871\n",
      "Epoch 000: Loss: 63.748\n",
      "0.01603360652923584\n",
      "Epoch 001: Loss: 49.088\n",
      "0.01435253381729126\n",
      "Epoch 002: Loss: 49.091\n",
      "0.013279361724853516\n",
      "Epoch 003: Loss: 49.057\n",
      "0.012523617744445801\n",
      "Epoch 004: Loss: 49.043\n",
      "0.01182307481765747\n",
      "Epoch 005: Loss: 49.076\n",
      "0.011107001304626465\n",
      "Epoch 006: Loss: 49.021\n",
      "0.011389490365982056\n",
      "Epoch 007: Loss: 49.087\n",
      "0.011728799343109131\n",
      "Epoch 008: Loss: 49.190\n",
      "0.013017270565032959\n",
      "Epoch 009: Loss: 49.158\n",
      "0.010681581497192384\n",
      "Epoch 010: Loss: 49.092\n",
      "0.010076498985290528\n",
      "Epoch 011: Loss: 49.275\n",
      "0.010208834409713746\n",
      "Epoch 012: Loss: 49.403\n",
      "0.009159275889396667\n",
      "Epoch 013: Loss: 49.105\n",
      "0.00829670250415802\n",
      "Epoch 014: Loss: 49.247\n",
      "0.008253355622291564\n",
      "Epoch 015: Loss: 49.196\n",
      "0.008552567362785339\n",
      "Epoch 016: Loss: 49.258\n",
      "0.010903799533843994\n",
      "Epoch 017: Loss: 49.224\n",
      "0.007897770404815674\n",
      "Epoch 018: Loss: 49.125\n",
      "0.007963371872901916\n",
      "Epoch 019: Loss: 49.359\n",
      "0.006701115965843201\n",
      "Epoch 020: Loss: 49.139\n",
      "0.006346144676208496\n",
      "Epoch 021: Loss: 49.078\n",
      "0.006913537383079529\n",
      "Epoch 022: Loss: 50.449\n",
      "0.00712231695652008\n",
      "Epoch 023: Loss: 49.162\n",
      "0.00597302258014679\n",
      "Epoch 024: Loss: 49.009\n",
      "0.004055046439170837\n",
      "Epoch 025: Loss: 48.956\n",
      "0.0028489777445793152\n",
      "Epoch 026: Loss: 49.000\n",
      "0.0049104815721511845\n",
      "Epoch 027: Loss: 49.020\n",
      "0.005131707191467285\n",
      "Epoch 028: Loss: 49.107\n",
      "0.0028703010082244875\n",
      "Epoch 029: Loss: 49.159\n",
      "0.007438197135925293\n",
      "Epoch 030: Loss: 49.867\n",
      "0.00443467527627945\n",
      "Epoch 031: Loss: 48.907\n",
      "0.002802671194076538\n",
      "Epoch 032: Loss: 49.114\n",
      "0.021103367805480958\n",
      "Epoch 033: Loss: 57.216\n",
      "0.011577985286712646\n",
      "Epoch 034: Loss: 49.168\n",
      "0.010389361381530762\n",
      "Epoch 035: Loss: 48.967\n",
      "0.011461930274963379\n",
      "Epoch 036: Loss: 48.942\n",
      "0.010497982501983643\n",
      "Epoch 037: Loss: 49.020\n",
      "0.009534999132156372\n",
      "Epoch 038: Loss: 49.047\n",
      "0.009466548562049865\n",
      "Epoch 039: Loss: 48.970\n",
      "0.008917269706726074\n",
      "Epoch 040: Loss: 49.019\n",
      "0.008151296377182007\n",
      "Epoch 041: Loss: 49.024\n",
      "0.007374463677406311\n",
      "Epoch 042: Loss: 49.015\n",
      "0.00711544394493103\n",
      "Epoch 043: Loss: 49.078\n",
      "0.006657180786132813\n",
      "Epoch 044: Loss: 48.996\n",
      "0.005975770354270935\n",
      "Epoch 045: Loss: 49.017\n",
      "0.00585709810256958\n",
      "Epoch 046: Loss: 49.029\n",
      "0.003800729811191559\n",
      "Epoch 047: Loss: 49.003\n",
      "0.0061216002702713015\n",
      "Epoch 048: Loss: 48.997\n",
      "0.013637592792510986\n",
      "Epoch 049: Loss: 49.834\n",
      "0.007595197558403015\n",
      "Epoch 050: Loss: 49.138\n",
      "0.006359428763389587\n",
      "Epoch 051: Loss: 48.979\n",
      "0.005442123413085937\n",
      "Epoch 052: Loss: 48.972\n",
      "0.0038098278641700745\n",
      "Epoch 053: Loss: 48.953\n",
      "0.0025812551379203797\n",
      "Epoch 054: Loss: 48.948\n",
      "0.002788707911968231\n",
      "Epoch 055: Loss: 48.952\n",
      "0.0028450974822044374\n",
      "Epoch 056: Loss: 48.978\n",
      "0.0055429601669311525\n",
      "Epoch 057: Loss: 49.187\n",
      "0.0033529150485992433\n",
      "Epoch 058: Loss: 48.951\n",
      "0.004100520610809327\n",
      "Epoch 059: Loss: 49.220\n",
      "0.003128664493560791\n",
      "Epoch 060: Loss: 48.960\n",
      "0.00271794855594635\n",
      "Epoch 061: Loss: 49.017\n",
      "0.004395485520362854\n",
      "Epoch 062: Loss: 49.005\n",
      "0.0036246258020401\n",
      "Epoch 063: Loss: 49.039\n",
      "0.00287850022315979\n",
      "Epoch 064: Loss: 48.978\n",
      "0.0037110289931297303\n",
      "Epoch 065: Loss: 49.127\n",
      "0.0031615519523620606\n",
      "Epoch 066: Loss: 49.089\n",
      "0.0038083499670028688\n",
      "Epoch 067: Loss: 49.002\n",
      "0.002556495666503906\n",
      "Epoch 068: Loss: 49.091\n",
      "0.002964569330215454\n",
      "Epoch 069: Loss: 48.976\n",
      "0.00448476642370224\n",
      "Epoch 070: Loss: 49.063\n",
      "0.0058717292547225955\n",
      "Epoch 071: Loss: 49.125\n",
      "0.0027982497215270994\n",
      "Epoch 072: Loss: 49.000\n",
      "0.003907793462276459\n",
      "Epoch 073: Loss: 49.170\n",
      "0.0028245824575424193\n",
      "Epoch 074: Loss: 48.960\n",
      "0.0028658798336982727\n",
      "Epoch 075: Loss: 49.068\n",
      "0.0026459047198295594\n",
      "Epoch 076: Loss: 49.122\n",
      "0.0029933130741119384\n",
      "Epoch 077: Loss: 48.975\n",
      "0.003037770390510559\n",
      "Epoch 078: Loss: 49.022\n",
      "0.0036135348677635193\n",
      "Epoch 079: Loss: 49.466\n",
      "0.004005318880081177\n",
      "Epoch 080: Loss: 48.949\n",
      "0.0036339980363845827\n",
      "Epoch 081: Loss: 48.981\n",
      "0.002994217276573181\n",
      "Epoch 082: Loss: 48.947\n",
      "0.003373739123344421\n",
      "Epoch 083: Loss: 48.966\n",
      "0.0029478353261947633\n",
      "Epoch 084: Loss: 49.010\n",
      "0.002505033314228058\n",
      "Epoch 085: Loss: 49.046\n",
      "0.003381873369216919\n",
      "Epoch 086: Loss: 49.150\n",
      "0.002779485583305359\n",
      "Epoch 087: Loss: 48.946\n",
      "0.003911457657814026\n",
      "Epoch 088: Loss: 48.969\n",
      "0.0036031347513198852\n",
      "Epoch 089: Loss: 49.031\n",
      "0.0030949050188064575\n",
      "Epoch 090: Loss: 49.029\n",
      "0.0026536959409713744\n",
      "Epoch 091: Loss: 49.002\n",
      "0.005421422719955445\n",
      "Epoch 092: Loss: 49.032\n",
      "0.003700779676437378\n",
      "Epoch 093: Loss: 49.049\n",
      "0.003204190731048584\n",
      "Epoch 094: Loss: 49.025\n",
      "0.0033232420682907104\n",
      "Epoch 095: Loss: 48.999\n",
      "0.018652180433273314\n",
      "Epoch 096: Loss: 57.482\n",
      "0.004620323181152344\n",
      "Epoch 097: Loss: 64.403\n",
      "0.0042104339599609375\n",
      "Epoch 098: Loss: 48.911\n",
      "0.003645264506340027\n",
      "Epoch 099: Loss: 48.985\n",
      "0.003926492631435395\n",
      "Epoch 100: Loss: 48.888\n",
      "0.003304482102394104\n",
      "Epoch 101: Loss: 48.962\n",
      "0.0037152743339538574\n",
      "Epoch 102: Loss: 48.928\n",
      "0.003030373156070709\n",
      "Epoch 103: Loss: 48.997\n",
      "0.0028015053272247314\n",
      "Epoch 104: Loss: 48.890\n",
      "0.003072165846824646\n",
      "Epoch 105: Loss: 48.900\n",
      "0.0031375014781951905\n",
      "Epoch 106: Loss: 48.943\n",
      "0.0030134016275405883\n",
      "Epoch 107: Loss: 48.893\n",
      "0.00312247633934021\n",
      "Epoch 108: Loss: 48.908\n",
      "0.0033977508544921877\n",
      "Epoch 109: Loss: 48.926\n",
      "0.003471654653549194\n",
      "Epoch 110: Loss: 48.923\n",
      "0.002890843451023102\n",
      "Epoch 111: Loss: 48.957\n",
      "0.0035075032711029052\n",
      "Epoch 112: Loss: 48.975\n",
      "0.0029440924525260927\n",
      "Epoch 113: Loss: 48.947\n",
      "0.0028450167179107665\n",
      "Epoch 114: Loss: 48.984\n",
      "0.0030341362953186036\n",
      "Epoch 115: Loss: 48.906\n",
      "0.003121728301048279\n",
      "Epoch 116: Loss: 48.978\n",
      "0.003179580867290497\n",
      "Epoch 117: Loss: 48.893\n",
      "0.0031555071473121643\n",
      "Epoch 118: Loss: 48.936\n",
      "0.002919964790344238\n",
      "Epoch 119: Loss: 49.006\n",
      "0.0034214717149734495\n",
      "Epoch 120: Loss: 48.957\n",
      "0.0031693831086158754\n",
      "Epoch 121: Loss: 48.983\n",
      "0.0025103801488876344\n",
      "Epoch 122: Loss: 48.969\n",
      "0.003043033480644226\n",
      "Epoch 123: Loss: 48.925\n",
      "0.002710071802139282\n",
      "Epoch 124: Loss: 48.945\n",
      "0.003650911450386047\n",
      "Epoch 125: Loss: 48.895\n",
      "0.0030839568376541137\n",
      "Epoch 126: Loss: 48.961\n",
      "0.0027748343348503114\n",
      "Epoch 127: Loss: 48.930\n",
      "0.00300800621509552\n",
      "Epoch 128: Loss: 48.986\n",
      "0.0029323253035545348\n",
      "Epoch 129: Loss: 48.978\n",
      "0.0029122388362884523\n",
      "Epoch 130: Loss: 48.952\n",
      "0.0030577215552330017\n",
      "Epoch 131: Loss: 48.932\n",
      "0.0042923265695571896\n",
      "Epoch 132: Loss: 48.933\n",
      "0.0030753040313720704\n",
      "Epoch 133: Loss: 48.962\n",
      "0.0038027352094650267\n",
      "Epoch 134: Loss: 49.002\n",
      "0.005021765232086182\n",
      "Epoch 135: Loss: 49.040\n",
      "0.0027730655670166015\n",
      "Epoch 136: Loss: 49.009\n",
      "0.006168530583381653\n",
      "Epoch 137: Loss: 49.012\n",
      "0.0044020313024520875\n",
      "Epoch 138: Loss: 48.971\n",
      "0.002660060524940491\n",
      "Epoch 139: Loss: 49.004\n",
      "0.003296516239643097\n",
      "Epoch 140: Loss: 48.989\n",
      "0.0032697975635528564\n",
      "Epoch 141: Loss: 49.049\n",
      "0.0033811479806900024\n",
      "Epoch 142: Loss: 48.986\n",
      "0.0033901453018188475\n",
      "Epoch 143: Loss: 48.980\n",
      "0.003975585103034973\n",
      "Epoch 144: Loss: 49.032\n",
      "0.002875122427940369\n",
      "Epoch 145: Loss: 48.978\n",
      "0.0032025310397148132\n",
      "Epoch 146: Loss: 48.938\n",
      "0.0029724299907684325\n",
      "Epoch 147: Loss: 49.001\n",
      "0.0031487831473350526\n",
      "Epoch 148: Loss: 48.994\n",
      "0.0031912988424301148\n",
      "Epoch 149: Loss: 48.950\n",
      "0.0030843049287796022\n",
      "Epoch 150: Loss: 48.996\n",
      "0.005121731162071228\n",
      "Epoch 151: Loss: 48.963\n",
      "0.004555453658103943\n",
      "Epoch 152: Loss: 48.954\n",
      "0.005503968000411987\n",
      "Epoch 153: Loss: 48.982\n",
      "0.0028265684843063353\n",
      "Epoch 154: Loss: 49.049\n",
      "0.003197517991065979\n",
      "Epoch 155: Loss: 48.984\n",
      "0.004963542520999909\n",
      "Epoch 156: Loss: 48.901\n",
      "0.0033191925287246703\n",
      "Epoch 157: Loss: 48.992\n",
      "0.0035725688934326173\n",
      "Epoch 158: Loss: 48.972\n",
      "0.004368602633476258\n",
      "Epoch 159: Loss: 48.999\n",
      "0.003357616066932678\n",
      "Epoch 160: Loss: 49.006\n",
      "0.0029710516333580016\n",
      "Epoch 161: Loss: 49.185\n",
      "0.002933855056762695\n",
      "Epoch 162: Loss: 48.932\n",
      "0.0033745691180229187\n",
      "Epoch 163: Loss: 48.948\n",
      "0.0035772013664245607\n",
      "Epoch 164: Loss: 48.933\n",
      "0.003941217362880707\n",
      "Epoch 165: Loss: 49.005\n",
      "0.0029988664388656615\n",
      "Epoch 166: Loss: 48.977\n",
      "0.0030313187837600706\n",
      "Epoch 167: Loss: 49.017\n",
      "0.0032502081990242004\n",
      "Epoch 168: Loss: 49.036\n",
      "0.005438097715377808\n",
      "Epoch 169: Loss: 48.979\n",
      "0.0030558618903160097\n",
      "Epoch 170: Loss: 48.964\n",
      "0.005404176115989685\n",
      "Epoch 171: Loss: 48.975\n",
      "0.0033315062522888184\n",
      "Epoch 172: Loss: 49.045\n",
      "0.0027175575494766237\n",
      "Epoch 173: Loss: 49.027\n",
      "0.0030365848541259767\n",
      "Epoch 174: Loss: 48.983\n",
      "0.0027973172068595886\n",
      "Epoch 175: Loss: 49.038\n",
      "0.0025779396295547486\n",
      "Epoch 176: Loss: 49.005\n",
      "0.003327668607234955\n",
      "Epoch 177: Loss: 49.007\n",
      "0.0030360051989555357\n",
      "Epoch 178: Loss: 48.978\n",
      "0.0031578147411346433\n",
      "Epoch 179: Loss: 48.988\n",
      "0.003404245674610138\n",
      "Epoch 180: Loss: 49.017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0027733880281448366\n",
      "Epoch 181: Loss: 49.004\n",
      "0.0036435902118682863\n",
      "Epoch 182: Loss: 48.969\n",
      "0.00461557388305664\n",
      "Epoch 183: Loss: 49.018\n",
      "0.005088064670562744\n",
      "Epoch 184: Loss: 48.986\n",
      "0.0030937135219573975\n",
      "Epoch 185: Loss: 48.973\n",
      "0.004570011496543885\n",
      "Epoch 186: Loss: 60.318\n",
      "0.0031241148710250853\n",
      "Epoch 187: Loss: 48.968\n",
      "0.003134901523590088\n",
      "Epoch 188: Loss: 48.945\n",
      "0.00330545037984848\n",
      "Epoch 189: Loss: 48.939\n",
      "0.003132765293121338\n",
      "Epoch 190: Loss: 48.917\n",
      "0.003526010513305664\n",
      "Epoch 191: Loss: 48.985\n",
      "0.0032968437671661377\n",
      "Epoch 192: Loss: 48.930\n",
      "0.004012938737869263\n",
      "Epoch 193: Loss: 48.985\n",
      "0.003410690426826477\n",
      "Epoch 194: Loss: 48.997\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-1c952145e4ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Run the model with real data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlower_bound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Lower bound u(0)=0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mupper_bound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Upper bound u(1)=1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m98\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Interior points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-c232a1a1214c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0;31m#print(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0;31m#f = model(x, training=True) # Estimate for u\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Find model gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Perform gradient decent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-4e15e4239d73>\u001b[0m in \u001b[0;36mloss_fn\u001b[0;34m(model, x)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;31m#print('f = ', f)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdf_dx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my_env/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1012\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my_env/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    373\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m  \u001b[0;31m# handle the corner case where self.layers is empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my_env/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtensors\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthere\u001b[0m \u001b[0mare\u001b[0m \u001b[0mmore\u001b[0m \u001b[0mthan\u001b[0m \u001b[0mone\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \"\"\"\n\u001b[0;32m--> 424\u001b[0;31m     return self._run_internal_graph(\n\u001b[0m\u001b[1;32m    425\u001b[0m         inputs, training=training, mask=mask)\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my_env/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my_env/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1012\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my_env/lib/python3.8/site-packages/tensorflow/python/keras/layers/core.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1207\u001b[0;31m     return core_ops.dense(\n\u001b[0m\u001b[1;32m   1208\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my_env/lib/python3.8/site-packages/tensorflow/python/keras/layers/ops/core.py\u001b[0m in \u001b[0;36mdense\u001b[0;34m(inputs, kernel, bias, activation, dtype)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my_env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my_env/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mbias_add\u001b[0;34m(value, bias, data_format, name)\u001b[0m\n\u001b[1;32m   3376\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3377\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3378\u001b[0;31m       return gen_nn_ops.bias_add(\n\u001b[0m\u001b[1;32m   3379\u001b[0m           value, bias, data_format=data_format, name=name)\n\u001b[1;32m   3380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my_env/lib/python3.8/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mbias_add\u001b[0;34m(value, bias, data_format, name)\u001b[0m\n\u001b[1;32m    672\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m    675\u001b[0m         _ctx, \"BiasAdd\", name, value, bias, \"data_format\", data_format)\n\u001b[1;32m    676\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run the model with real data\n",
    "train()\n",
    "lower_bound = tf.zeros(shape=[1,1]) # Lower bound u(0)=0\n",
    "upper_bound = tf.ones(shape=[1,1]) # Upper bound u(1)=1\n",
    "x = tf.sort(tf.random.uniform(shape=[1,98])) # Interior points\n",
    "test_points_tensor = tf.transpose(tf.concat([lower_bound, x, upper_bound], 1)) # Actual data\n",
    "test_points = test_points_tensor.numpy()\n",
    "g = model(test_points_tensor, training=False).numpy()\n",
    "x_actual = tf.sort(tf.random.uniform(shape=[1,1000]).numpy()) # Known solution\n",
    "#print(test_points[0])\n",
    "#print(x_actual[0])\n",
    "#print(f[0])\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title('1D Laplace (DVM)')\n",
    "ax.plot(x_actual[0],x_actual[0], label='u(x)', color='blue')\n",
    "ax.plot(test_points,g,'r--', label='g(x)')\n",
    "ax.legend()\n",
    "ax.set_aspect('equal','box')\n",
    "\n",
    "tikzplotlib.save(\"1d_laplace_dvm.tex\")\n",
    "\n",
    "average_error = np.sum(abs(g-test_points))/len(g)\n",
    "print(average_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "concerned-france",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005584139823913575\n",
      "Epoch 000: Loss: 331.457\n",
      "0.0053451406955718995\n",
      "Epoch 001: Loss: 48.976\n",
      "0.005005162954330444\n",
      "Epoch 002: Loss: 49.014\n",
      "0.004741934537887574\n",
      "Epoch 003: Loss: 49.014\n",
      "7.808128704999945\n"
     ]
    }
   ],
   "source": [
    "print(timeit.timeit(train, number=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
