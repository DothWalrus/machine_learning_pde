{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "vietnamese-credit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "import tikzplotlib\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "little-laugh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section is just me figuring out how tensors work\n",
    "#lower_bound = tf.zeros(shape=[1,1]) # Lower bound u(0)=0\n",
    "#upper_bound = tf.ones(shape=[1,1]) # Upper bound u(1)=1\n",
    "#x = tf.random.uniform(shape=[1,100]) # Interior points\n",
    "#training_pts = tf.concat([lower_bound, x, upper_bound], 1) # Actual training data\n",
    "#x_0 = tf.slice(training_pts, [0,0], [1,1])\n",
    "#x_1 = tf.slice(training_pts, [0,101], [1,1])\n",
    "#print(x_0.numpy(), x_1.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "returning-superintendent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct Simple Feedforward Network\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_shape=[1], activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "#model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "placed-venture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Loss Function\n",
    "def loss_fn(model,x):\n",
    "    #print(x)\n",
    "    with tf.GradientTape() as t:\n",
    "        t.watch(x)\n",
    "        f = model(x, training=True)\n",
    "        #print('f = ', f)\n",
    "    df_dx = t.gradient(f,x)\n",
    "    u_0 = tf.zeros(shape=[1,1]) # Lower boundary condition\n",
    "    u_1 = tf.ones(shape=[1,1]) # Upper boundary condition\n",
    "    bound_weight = 1000 # lambda = sqrt(bound_weight)\n",
    "    mse = tf.keras.losses.MeanSquaredError()\n",
    "    lower_bound_error = tf.math.subtract(tf.slice(f,[0,0],[1,1]),u_0)\n",
    "    #print(lower_bound_error)\n",
    "    upper_bound_error = tf.math.subtract(tf.slice(f,[99,0], [1,1]),u_1)\n",
    "    loss_1 = tf.math.add(tf.nn.l2_loss(df_dx), \n",
    "                       tf.nn.l2_loss(bound_weight*lower_bound_error))\n",
    "    loss = tf.math.add(loss_1, tf.nn.l2_loss(bound_weight*upper_bound_error))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "straight-fashion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_for():\n",
    "    # Train network\n",
    "    optimizer = tf.keras.optimizers.Adam() # Fancy gradient decent\n",
    "    epochs = 20\n",
    "    train_loss_results = [] # For tracking loss during training\n",
    "    iterations_per_epoch = 100\n",
    "    minibatch_size = 100 # Number of points to be selected each iteration\n",
    "\n",
    "    for epoch in range(epochs): # Uncomment to run for specific number of epochs\n",
    "        epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "        for iteration in range(iterations_per_epoch):\n",
    "            lower_bound = tf.zeros(shape=[1,1]) # Lower bound u(0)=0\n",
    "            upper_bound = tf.ones(shape=[1,1]) # Upper bound u(1)=1\n",
    "            interior_pts = tf.random.uniform(shape=[1,minibatch_size-2]) # Interior points\n",
    "            x = tf.transpose(tf.concat([lower_bound, interior_pts, upper_bound], 1)) # Actual training data\n",
    "            with tf.GradientTape() as t:\n",
    "                t.watch(x)\n",
    "                #print(x)\n",
    "                #f = model(x, training=True) # Estimate for u\n",
    "                loss = loss_fn(model,x) # Loss\n",
    "            grads = t.gradient(loss, model.trainable_weights) # Find model gradients\n",
    "            optimizer.apply_gradients(zip(grads,model.trainable_weights)) # Perform gradient decent\n",
    "            epoch_loss_avg.update_state(loss) # Track loss\n",
    "            # End training iteration\n",
    "        train_loss_results.append(epoch_loss_avg.result())\n",
    "        #print(\"Epoch {:03d}: Loss: {:.3f}\".format(epoch, epoch_loss_avg.result()))\n",
    "        # End Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "excited-writing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_while():\n",
    "    # Train network\n",
    "    optimizer = tf.keras.optimizers.Adam() # Fancy gradient decent\n",
    "    epochs = 20\n",
    "    train_loss_results = [] # For tracking loss during training\n",
    "    iterations_per_epoch = 100\n",
    "    minibatch_size = 100 # Number of points to be selected each iteration\n",
    "    average_error = 1\n",
    "    epoch = -1\n",
    "    while average_error > 0.005: \n",
    "        epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "        for iteration in range(iterations_per_epoch):\n",
    "            lower_bound = tf.zeros(shape=[1,1]) # Lower bound u(0)=0\n",
    "            upper_bound = tf.ones(shape=[1,1]) # Upper bound u(1)=1\n",
    "            interior_pts = tf.random.uniform(shape=[1,minibatch_size-2]) # Interior points\n",
    "            x = tf.transpose(tf.concat([lower_bound, interior_pts, upper_bound], 1)) # Actual training data\n",
    "            with tf.GradientTape() as t:\n",
    "                t.watch(x)\n",
    "                #print(x)\n",
    "                #f = model(x, training=True) # Estimate for u\n",
    "                loss = loss_fn(model,x) # Loss\n",
    "            grads = t.gradient(loss, model.trainable_weights) # Find model gradients\n",
    "            optimizer.apply_gradients(zip(grads,model.trainable_weights)) # Perform gradient decent\n",
    "            epoch_loss_avg.update_state(loss) # Track loss\n",
    "            # End training iteration\n",
    "        # Check average error    \n",
    "        lower_bound = tf.zeros(shape=[1,1]) # Lower bound u(0)=0\n",
    "        upper_bound = tf.ones(shape=[1,1]) # Upper bound u(1)=1\n",
    "        x = tf.sort(tf.random.uniform(shape=[1,98])) # Interior points\n",
    "        test_points_tensor = tf.transpose(tf.concat([lower_bound, x, upper_bound], 1)) # Actual data\n",
    "        test_points = test_points_tensor.numpy()\n",
    "        g = model(test_points_tensor, training=False).numpy()\n",
    "        average_error = np.sum(abs(g-test_points))/len(g)\n",
    "        print(average_error)\n",
    "        \n",
    "        train_loss_results.append(epoch_loss_avg.result())\n",
    "        epoch+=1 \n",
    "        print(\"Epoch {:03d}: Loss: {:.3f}\".format(epoch, epoch_loss_avg.result()))\n",
    "        # End Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "crucial-mortality",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01918726921081543\n",
      "Epoch 000: Loss: 34389.535\n",
      "0.021067743301391603\n",
      "Epoch 001: Loss: 51.825\n",
      "0.019925408363342285\n",
      "Epoch 002: Loss: 51.801\n",
      "0.017142375707626344\n",
      "Epoch 003: Loss: 51.555\n",
      "0.015679073333740235\n",
      "Epoch 004: Loss: 51.305\n",
      "0.014307026863098144\n",
      "Epoch 005: Loss: 51.024\n",
      "0.014370460510253907\n",
      "Epoch 006: Loss: 50.765\n",
      "0.010900721549987794\n",
      "Epoch 007: Loss: 50.636\n",
      "0.008479062914848328\n",
      "Epoch 008: Loss: 50.162\n",
      "0.006686753630638123\n",
      "Epoch 009: Loss: 50.810\n",
      "0.006636239886283875\n",
      "Epoch 010: Loss: 49.942\n",
      "0.006756807565689087\n",
      "Epoch 011: Loss: 49.846\n",
      "0.0052514588832855225\n",
      "Epoch 012: Loss: 51.260\n",
      "0.007859122753143311\n",
      "Epoch 013: Loss: 50.132\n",
      "0.00557978093624115\n",
      "Epoch 014: Loss: 49.772\n",
      "0.0059672611951828\n",
      "Epoch 015: Loss: 53.542\n",
      "0.007546294927597046\n",
      "Epoch 016: Loss: 52.624\n",
      "0.0020333418250083925\n",
      "Epoch 017: Loss: 51.242\n",
      "0.002020779401063919\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAEICAYAAAB/KknhAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAkUUlEQVR4nO3de5yMdf/H8dfHUqsShboTRd1sdGaTUs66SahIVtyUUDlt6ZwQ1Z3YppP7IHVHd0KtopBDOZRfhIocUkhsyFpn2nXN7uf3x8zWWnsYzM41h8/z8ZiHmev6znW9Z9Znvtf1neu6RlQVY0xsKeV2AGNM6FnhGxODrPCNiUFW+MbEICt8Y2KQFb4xMcgKP8aJSA8R+dKF9fYRkZdDvV7/ulNE5H431h0urPBLiIj0E5HlIpIlIm/nm9dERHJE5KD/liYiU0TkmiKWV11EVERKl3j4EiYipwCDgVH+x7mvLff9+E1EPhGRlv758SKyV0SaFbAsj4h84L+/WUSOiEilfG2+9S+/un/SaOBJf46YZIVfcrYBzwJvFTZfVc8AygENgB+AL0SkeYjyuak98IOq/ppvegX/e3IlMBf4UER6qGomMBn4e97GIhIHJAHj80z+2T8tt83lwGl5n6eq2/G93+2C83IijxV+CVHVqar6EZBRTDtV1TRVHQKMA0Ye77pEpL6IfOXvFbeLyOt5ezN/bzdARDaJyC4RGSUiBf7tReQVEdkqIvtFZIWI3JhnXpyIPCkiG0XkgH9+Nf+8S0RkrojsFpH1ItKpiMitgYVFvCc7VPUVYBgw0p91PNBBRPIW8d/w/R+elWfaOxz9AdEdmFDAahYAbYrIGNWs8MPLVKCuiJx+nM/LBh4EKgHXAc2BB/K1uQ1IBOri63HvKWRZy4CrgLOBicD7IhLvn/cQvt70ZuBM/zIO+/PO9bc/B+gM/FNE6hSyjsuB9QG8rqn+5SWo6v8B24Hb88zvBkxUVW+eaUuAM0Wktn+LoDPwvwKWvQ7flkVMssIPL9sAASocz5NUdYWqLlFVr6puBv4DNM7XbKSq7lbVLcDL5Nkczres/6lqhn9ZKcCpQIJ/9r3AYFVd799SWamqGcAtwGZV/a//ed8CqcAdhUSuABwI4KVt8/97tv/fCfh7cxE5E98H2PgCnpfb67fEV+D5dynwr79CABmiUsQPFEWZ8wEF9h7Pk0SkFvASvh79NHx/1xX5mm3Nc/8XoEohy3oY6Omfr/h69tzBsmrAxgKediFwrYjkzV0aXwEWZA++sY3inO//d7f/33eAoSJSBWgFbPR/yOT3DrAIqEHBm/n417+3kHlRz3r88HIb8I2qHjrO5/0L32BVTVU9E3gS35ZDXtXy3L+AP3vTP/j35x8FOgFnqWoFYF+eZW0FLi5g/VuBhapaIc/tDFUt7CuzVUCtAF7XbcBO/LsFqvoL8AXQFd9mfkG9fW67n/HtkkwtZNm1gZUBZIhKVvglRERK+/eN44A4/1dSx2xhic/5IjIU36b0k8Us+lT/snJvpfD1XvuBgyJyCVBQwT0iImf5B+MG4hslz68c4AXSgdIiMgRfj59rHDBCRGr6c18hIhWBT4BaItJNRMr4b9eISO1CXsNMjt0V+YOInCsi/YChwBOqmpNn9nigH9AQeLewZeDbamlWxIdoY44eFIwtqmq3ErjhG5HWfLdh/nlNgBzgIHAIX+/7AdCgiOVVL2B5CrQAGuHr8Q/i6xGHA1/mea4CA4BN+L5lSAHi/PN65LbF9yH1Fr4Pke34ev/NQIs88wfj600P4BsIrOqflwDMwPehkQF8DlxVyGspA2wBquR7bbnvx058Hw6tCnjuGf52swqY90fWfNNL+5df3f/4PCANOMXt/ydu3cT/RpgoJiKKbzdgg9tZcolIb6COqia7sO4UfOMD/wz1usOFFX4MCMfCN+6yfXxjYpD1+MbEIOvxjYlBrh3AU6lSJa1evbpbqzcmJqxYsWKXqlbOP921wq9evTrLly93a/XGxAQR+aWg6bapb0wMssI3JgZZ4RsTg8Lq7DzHcUhLSyMzM9PtKEEXHx9P1apVKVOmjNtRjAmvwk9LS6NcuXJUr14dkfwnl0UuVSUjI4O0tDRq1Kjhdhxjit/UF5G3RGSniKwuZL6IyKsiskFEVolI3RMNk5mZScWKFaOq6AFEhIoVK0blloyJTIHs47+N76IHhWkN1PTfeuM7N/yERVvR54rW12UiU7GFr6qL+PMKKAVpD0xQnyVABRE5L1gBjTF/co4ojw84zLZjLqNyfIIxqn8+R1/WKY0/L5l0FBHpLb5rzS9PT08PwqqNiR2OAxMTX6LDa435bNrBk1pWSL/OU9WxqpqoqomVKx9zFGHYS05OZtGiRUW2adGiBXv27AlRIhMrHAeSkuCp7+/E+7db6NbntOKfVIRgFP6vHH09t6oUfFXTiJaRkcGSJUto1KhRke26devGP/8Zs9d3MCXAycrh39dP4MPUbB72VOW6T4dCqZMr3WB8nTcd6Ccik4BrgX3q+6WSk5KcDN99d7JLOdpVV8HLLxfdZvPmzdxyyy2sXu37EmP06NEcPHiQKlWq0KqVb4xz37591K9fn+nTp5OQkEBSUhLNmjWjV69etGvXjhtvvJGnnnoquOFNTHKycphf+376/zyWi/4eT5vkon6nJHCBfJ33HvAVkCC+33jrKSL3ich9/iYz8V3LbQPwBsf+kENUWLx4MfXq1QOgfPnyvP766/To0YNJkyaxZ88eevXqBcBZZ51FVlYWGRlF/oCOMcVyMrNZdEkvbvp5LF+3eJI2bxf2MwXHr9geX1UL/OGFPPMV6Bu0RH7F9cyhtn37dvKOS7Rs2ZL333+fvn37snLl0VdpPuecc9i2bRsVK1YMdUwTJZzMbBYn3EPzLRNYctMQGnw6DIL4lbAdq59P6dKlycn582rOuQfdlC1b9qgDcHJycli3bh2nnXbaMYN5mZmZlC1bNjSBTdRxHHi07Trqb3mf/2s9ggaznwlq0YMV/jHOPfdcdu7cSUZGBllZWXzyyScA1K5dmw0b/rxWpcfjoXbt2kycOJG7774bx3EA3+G5O3bswC4yYk6Ec0RJSoKX513Ge0N+4PqZg0tkPVb4+ZQpU4YhQ4ZQv359WrZsySWXXAJAmzZtWLBgAQDr169n3LhxpKSkcOONN9KoUSOeffZZAFasWEGDBg0oXTqsToMwEcD53cvSi7tQKfXfeDzQ85kLSm5lbl3Qv169eprf2rVrj5kWTho2bKh79uwpss2AAQN03rx5Bc4L99dn3HPk0BFdfP4dqqAL240K2nKB5VpA/VmPfxxSUlLYsmVLkW0uu+wymjdvHqJEJho4hx2W10ri+l/fZ2H7FBpNe7jE12nbo8fh2muvLbZN7td6xgTCycphRc3OXLdtKgtv9dD4w+SQrNcK3xiXOA4k3VWK6tuuI/O2xjSZOiBk67ZNfWNc4Bw6wiNt1pCaClU9D4e06MF6fGNCzjmYxcqaHRm24wtqj/iJPsmhP2HNevzjYGfnmZPlHMhkVc0OJO74hBV3jKTPYHfOUrXCD5CdnWdOlnMgk9U1b6PejhnM6zSW5lP6uJbFCr8AI0aMICEhgRtuuIGkpCRGjx5NamrqUWfnJSQksH79egCSkpJ44403AGjXrh3vvfeea9lNeHIcmHKdhyt/m83cO8fRYrK73/6E9z5+kybHTuvUCR54AA4fhptvPnZ+jx6+265d0LHj0fP8R94VZdmyZaSmprJy5Uocx6Fu3brUq1ePxYsX09G/vLxn5w0cOLDQs/PsJB0Df15EY9qahyl7XyK3/6ul25Gsx89v8eLFtG/fnvj4eMqVK0fbtm2Bgs/Ou/zyy+nbty/jxo07ahm5Z+cZ4+w7zOcJ97ModSejPGXCough3Hv8onro004ren6lSgH18IEq7uy8qlWr/jHPzs4zAM7eQ/xQqy0t0xfw5t030Tb5Nrcj/cF6/HwaNmzIxx9/TGZmJgcPHrSz88wJcfYeYn2tW6iTvpA5d02g7VvhU/RghX+Ma665hnbt2nHFFVfQunVrLr/8csqXL29n55mAOXsO8mPNm6mdvog5Xd+h1f+6uh3pWAWduROKWzifnXfgwAFVVT106JDWq1dPV6xYoap2dp4p3pEjqve02aHfc6nO6Pae23EKPTvPuqUC9O7dm7Vr15KZmUn37t2pW9f3q2C5Z+dVqFCh0Ofa2Xmxy9lzkLt6xvP+jHO5cvS3DBgUvj+QaoVfgIkTJxY43c7OM4VxMvazMaE1bTMu4nrPOwxIDt+ihzDcx/dtnUSfaH1dBpxd+9hU629cnPE1Z/W4leRktxMVL6wKPz4+noyMjKgrEvX/THZ8fLzbUUyQObv28XPC37ho93I+vXsKt/y3g9uRAhJWm/pVq1YlLS2NaPxdvfj4+KO+6zeRzzmirKvTgUt2f8Osez6g3Zvt3Y4UsLAq/DJlylCjRg23YxhTLMeBpC7CjvRhPNpzN+3GtXM70nEJq019YyKB89tuxjScSGoqdPTcEHFFD2HW4xsT7pwdGWyt3ZL79q6l3JAb6JlcgpfALkFW+MYEyNm+i7TaLaiy7wdm9f6oZK97X8Ks8I0JgLN9F7/Wbs55+9Yzq880bvv339yOdFJsH9+YYjgO/LP9bM7Z9xMz+0yP+KIH6/GNKZJzREnqIqQuu4vThzbm3mHR8ZWs9fjGFMJJ+43Nf2nAjtQv8XiImqKHAAtfRFqJyHoR2SAijxcw/wIRmS8i34rIKhEp4JpYxkQOZ+sOdtRpSpU9q0nu642Iw3CPR7GFLyJxwBigNVAHSBKROvmaDQamqOrVQGfALjNrIpazZTu/XdqUsw5sYWa/WXR8vYnbkYIukB6/PrBBVTep6hFgEpD/2EQFzvTfLw/YBedMRHK2pbPz0iZUOLCVmf1mccdrRV9OPVIFMrh3PrA1z+M0IP/5qcOAOSLSHzgdaFHQgkSkN9Ab4IILIvc7UBOdHAfu6ncWLQ82onz/7nR69Qa3I5WYYA3uJQFvq2pV4GbgHRE5ZtmqOlZVE1U1Me8Va41xm/NzGg/cuo33PyzNIc8bUV30EFiP/ytQLc/jqv5pefUEWgGo6lciEg9UAnYGI6QxJcnZuIWMK5vS61BFLn1pKcnJ4nakEhdIj78MqCkiNUTkFHyDd9PztdkCNAcQkdpAPBB959aaqONs+IXdVzSh7KFdbBr4KskPRn/RQwCFr6peoB8wG1iHb/R+jYgMF5Hc05IGAb1EZCXwHtBDo+1qGibqOD9tZveVTTj18G5mJs+l88sN3I4UMgEduaeqM4GZ+aYNyXN/LdAwuNGMKTmOAysb9efiw3uZ+eA8uryU6HakkLJDdk3Myf0tu0U73sTz8DbuGnWV25FCzg7ZNTHFWb+Jzy7py7RUhyc958Rk0YMVvokhzroN7Lu6MddsmsQbT26OusNwj4cVvokJztqf2F+vCfz+O7Me/pwez9V0O5KrbB/fRD1nzY/sT2xKTuYRPn1kPt1evNztSK6zHt9ENceBwf33kZF5Op8+akWfy3p8E7Wc33aT1PdsUudfQ5XRaxk4yP6757Ie30QlZ+VaDl5Yh3NTx+DxYEWfj70bJuo4363hUINmZGaVov7jzeme7Hai8GM9vokqzrerOdygKYez4pjzxAK6/+MStyOFJevxTdRwdu3j0HXNOZRVhrlPzKfH87XcjhS2rPBNVHAcSLqvPKdnvUjTJ6+P+e/pi2Ob+ibiOV9/y7PN55OaCld7ulvRB8B6fBPRnK+/IfOGFnR2zuXs0d8zMNn+SwfCenwTsZylK8i8oQUZzpl8NXimfWV3HKzwTUTyfrWMrBtbsMspz+dPL+CeETXcjhRRrPBNxHEcmNt1PDuds3xFP7y625EijhW+iShOVg5JSXDLpleYO3wJPYdf6HakiGSFbyKG94uv2PqXRJakppHiiaPP0+e4HSli2WiIiQjehYtxWrTC6z2PYUPh3mS3E0U26/FN2PMu+BKnRSu2eKuwcOj8qPrVWrdYj2/CmnfxUpwWrfgluypfDPucXkOruB0pKljhm7DlONBrZE1aZd/C/mEeeg89z+1IUcM29U1Ycr7+hm6dshj/8dns8Eyyog8y6/FN2PHO/oycNm1pkN2bBp6XY/pquCXFCt+EFe+n88hu05Yfc/5K2eFP0ifZ7UTRyTb1TdjwzpxDdpu2/JBTi/8b8bl9T1+CrMc3YcHZ/zv7O9zN1pwEloyYx32DK7kdKapZ4RvXOQ4k3VOWnzJncv+zVbnvqYpuR4p6tqlvXOWdNoMpV/+D1FS423OlFX2IBFT4ItJKRNaLyAYRebyQNp1EZK2IrBGRicGNaaKR98OP0dtuI2FNKq++mGmj9yFU7Ka+iMQBY4CWQBqwTESmq+raPG1qAk8ADVV1j4jYqIwpknfqdLRjR77Vq1jx/Bz6PxLvdqSYEkiPXx/YoKqbVPUIMAlon69NL2CMqu4BUNWdwY1poon3g4/Qjh35Rq9mxfNzuP+JCm5HijmBFP75wNY8j9P80/KqBdQSkcUiskREWhW0IBHpLSLLRWR5enr6iSU2Ec1x4N8v7GWZJlrRuyhYg3ulgZpAEyAJeENEKuRvpKpjVTVRVRMrV64cpFWbSOFs30VSEvRf0YNlKV/wwBPl3Y4UswIp/F+BankeV/VPyysNmK6qjqr+DPyI74PAGAC8772P94IabE9d7Pstu4fi3I4U0wIp/GVATRGpISKnAJ2B6fnafISvt0dEKuHb9N8UvJgmknnfnYzclcRy71V0+ccVNnofBootfFX1Av2A2cA6YIqqrhGR4SLSzt9sNpAhImuB+cAjqppRUqFN5PC+8x7StQuL9XpWvTCLvo+XczuSAURVXVlxYmKiLl++3JV1m9DwfrkEubEhX3Aja174hL6PneF2pJgjIitUNTH/dDtk15QIx4EunvpUZTQXv9Cbfo+d7nYkk4cdsmuCzvvuZAa028wHU0txoedBK/owZD2+CSrvuP9SqldPrqEHCZ63bCAvTFmPb4LG+8ZblOrVk7m05NDIMVb0YcwK3wSF9z/jKN27J3O4iR9fnEb/R8u6HckUwTb1zUlzfvey+Yk3+InWbHxxqp1wEwGsxzcnxcnKIalbaa7ZM5uNoz60oo8QVvjmhGW/OoY1F7RmRurvDPNUoP/Dp7odyQTICt+ckOyXXyNuYD827yzLyFFxNpAXYazwzXHLfukV4h4cwIfcypZRUxjw8CluRzLHyQrfHJfs1/9F3KBkpnIbW0dNtqKPUDaqbwLmOPDYtBu4hF5kjh7DgEFl3I5kTpD1+CYg3vlfkNRZ8cy7nMOesVb0Ec4K3xQr+/mRlG7WiFJT38fjwQbyooAVvilS9ojniXvqcSaSRMPRt1vRRwkrfFOo7GeeJW7IU7xDV9JT3mHgIBsSihZW+KZAzrerYdhQJtCN3Slv2zXyoox9hJtjOA4kPXcZ21lEp5QGVvRRyHp88ydVsocOZ3Tjj0lNhTs8Da3oo5QVvvFRJXvwUOKGD6XCVzNt9D7K2aa+8RX9U0OI+8ezjKMnWSl2EY1oZz1+rMtT9G9wLwdTxpL8kP23iHbW48c4x4F5k/eQxr0cSvmPFX2MsL9yDHN+201SF+HmTa9Z0ccY+0vHqOynh7G/xhX8X+o2PB6xoo8x9teOQdlDniHu2WeY9vtNPJryFxvIi0FW+DEme9gI4kYM47/0YH/KOOvpY5T91WOI9823iXtmCOP5uxV9jLNR/RjhOHDP9Nupynb+kvKoHZEX4+wjPwZ4x79LjzsO8b/pZ3Ku5wkremOFH+2yn/0HpXt0pdq01+wwXPOHgApfRFqJyHoR2SAijxfRroOIqIgc83vcJvSyn3uBuKef5F26cF7KI1b05g/FFr6IxAFjgNZAHSBJROoU0K4cMBBYGuyQ5vhlPz+SuMFP8C5d2JUywTbvzVEC6fHrAxtUdZOqHgEmAe0LaDcCGAlkBjGfOQHOb7vZP9zDRJLYNXq8Fb05RiCFfz6wNc/jNP+0P4hIXaCaqs4oakEi0ltElovI8vT09OMOa4rnOJDU92yuylpK+ugJdrksU6CTHtwTkVLAS8Cg4tqq6lhVTVTVxMqVK5/sqk0+2SNHM+OyR0lNVR70XGhFbwoVSOH/ClTL87iqf1qucsBlwAIR2Qw0AKbbAF9oZb+YQtzjj5D54xZeTsmxgTxTpEAKfxlQU0RqiMgpQGdgeu5MVd2nqpVUtbqqVgeWAO1UdXmJJDbHyB71EnGPPcxkOvHb6P/ZPr0pVrGFr6peoB8wG1gHTFHVNSIyXETalXRAU7Ts0R7iHh3EFO5gx+h3bfPeBCSg/yWqOhOYmW/akELaNjn5WCYQjgOvfFCNKrmj91b0JkB25F6EctZvIikJHlnakZ2eifZbdua4WOFHoOyXXkFqJ7A9dbEdhmtOiBV+hMl++TXiBiUzXdty5+j6VvTmhFjhR5DsV8cQ9+AAPuRW0kZNss17c8Ks8COEd+Fi4gb24yPas3XUZAY8fIrbkUwEs2HgCOA4kPTq9ZTjLa4edZcVvTlp1uOHOe/4dxnU5gdSpwpXeu62ojdBYYUfxrxvjqdUj25cM/c5G703QWWFH6a849+l1L138znN2DtyrBW9CSor/DDkfXcy0uPvLKQxP4ycTv9Hy7odyUQZG9wLM84RZd2Db7CHG1jzwif0e/Q0tyOZKGQ9fhhxjihJXYTr0qex+oUZ9HvsdLcjmShlhR8mvB9+zA9VmzM79QDPeU6n72NnuB3JRDEr/DDgnTYD7dCB39MPMvJ5u4iGKXlW+C7zzpiN3n47K/UKVjw/hweeKO92JBMDbHDPRd45n5PTrj1rcuqw7Nk53P9EBbcjmRhhPb5LHAeSU6oxL6cZS0fM5f6nznY7kokhVvgucNb+RFJnZcycmvzomcl9gyu5HcnEGCv8EPMu+BLvFVdTc+oLdhiucY3t44eQ94uvcFq2Zkv2+dQY1oPeyW4nMrHKCj9EvIuXcqTZ3/jV+xcWDf2c3kPPczuSiWFW+CHg7DnI4eZt2eWtzMKh8+k17Pzin2RMCbLCL2GOA0m9zuBg1gTuGFKHe4dVdTuSMTa4V5Kc5St55cYPSE2FVp5W9HzmArcjGQNYj19inG++5/eGzelwpBynvngL/ZPj3Y5kzB+s8EuA890aDl/fnANH4lnw5Fz6P2JFb8KLFX6QOavWcahBMw5nlWbek/O5+7m/uh3JmGPYPn4QOQ5MvmsamVnC3Cfm0+O5mm5HMqZAVvhB4hxRkpKg2+rHmDZ8Fd2fT3A7kjGFssIPAmf9Jracm8j61O/xeIQ+T5/jdiRjihRQ4YtIKxFZLyIbROTxAuY/JCJrRWSViHwmIhcGP2p4cn7azN66TamwdzOPPaJ27L2JCMUWvojEAWOA1kAdIElE6uRr9i2QqKpXAB8ALwY7aDhyNm5h91VNiTt8gE8HzaPri1e4HcmYgATS49cHNqjqJlU9AkwC2udtoKrzVfWw/+ESIOoPT3N+2UbGlU059fAePn1oLneNvtrtSMYELJDCPx/Ymudxmn9aYXoCswqaISK9RWS5iCxPT08PPGWYcRzoPrACXx66mlnJc+iSUs/tSMYcl6AO7olIVyARGFXQfFUdq6qJqppYuXLlYK46ZJytO+jZcR/vTTuNNM8HJHnqux3JmOMWyAE8vwLV8jyu6p92FBFpATwFNFbVrODECy9O2m/suLQZ3Q+cR92X5pGcLG5HMuaEBNLjLwNqikgNETkF6AxMz9tARK4G/gO0U9WdwY/pPmdbOtsvbU7FA5vZ3XcIyQ9a0ZvIVWzhq6oX6AfMBtYBU1R1jYgMF5F2/majgDOA90XkOxGZXsjiIpKzI4Nf67Sg8v6NzHzgE+54vbHbkYw5KQEdq6+qM4GZ+aYNyXO/RZBzhQ3Hge/q3sPl+9Yz876P6TimmduRjDlpdpJOERwHkpJgxfaXSblvA7f/q6XbkYwJCjtktxBOxn4m1kthamoOAz01rOhNVLEevwDO7gNsTGhNl4yvKfNQE7ok2/f0JrpYj5+Ps/cQPyW04a8ZS5ndY5IdnGOikhV+Hs6+w/xQqy0JuxYz5+/vcst/O7gdyZgSYYXv5zgw7NbvuDh9CXO6TuDm8Xe6HcmYEmP7+PgvotFFSF1wPRcO/5neT5/rdiRjSlTM9/jOwSxWXtiO01PH4/FgRW9iQkwXvnMwi1U1O5C44xO63unYRTRMzIjZwncOHeG7Wp2ot2MGn93xb1pOutftSMaETEwWvpOZzTe1OnPN9ul83uF1mk/p43YkY0Iq5gb3HAeSusZRZ9tVHL6tKc0+6Ot2JGNCLqYK38nMZuCtv5A6+yJu8AyhabLbiYxxR8wUvpOVw+KEexix5ROuGr6W3sk2em9iV0zs4ztZOSy8pA9NtkxgfesH7Ss7E/OivvCdI8pndfrTYvM4lrYczPUzB7sdyRjXRXXhOw68ed04Wm36J8ubPsK1s4e7HcmYsBC1+/i5F9H45JtuXHyn0PK9niB2nTxjIEp7fMeBsdf9l89Td/OCJ953cI4VvTF/iLrCdxyYWC+FvivuYeZNr9hhuMYUIKoK33FgfP0xdP/+YX688g4azHja7UjGhKWoKXzHgTcavMm93/Vj42XtqbXsXSgdtUMYxpyUqCh8x4G/d8qk5TcvsPmSVly8fDKUKeN2LGPCVsQXfu7o/aSP4lkwbCHVv5kKp57qdixjwlpEbws7DrzU6CNuWjKTG1P+Ra+HqrgdyZiIELGF7zgwuvHHDFrSiYwL63Fe79/x/YqXMaY4Ebmp7zjwUuNpDPqqA7svuIrzvpsFZ1jRGxOoiCt8x4GXG03loa86knFhXf6yai5UqOB2LGMiSkQVfu5A3qwlFdheszHnrZoD5cu7HcuYiBMxhe848OjNq0lNhXaeZlywfi6ceabbsYyJSBFR+I4Dk+q+SMq8K0jtM8d3GK4de2/MCQuo8EWklYisF5ENIvJ4AfNPFZHJ/vlLRaR6sAI6R5QZlz1Gt9WP8dPVd3L7a02DtWhjYlaxhS8iccAYoDVQB0gSkTr5mvUE9qjqXwEPMDIY4ZzMbBYm9ObWH19kZcP7SVj2Pzsiz5ggCKTHrw9sUNVNqnoEmAS0z9emPTDef/8DoLnIyW2Lq8Lolp/+ceWcK78YA3FxJ7NIY4xfIIV/PrA1z+M0/7QC26iqF9gHVMy/IBHpLSLLRWR5enp6kSsVgXPvbsOkgV9x7ZwRtk9vTBCF9Mg9VR0LjAVITEzU4trfcw9AgxJOZUzsCaTH/xWoludxVf+0AtuISGmgPJARjIDGmOALpPCXATVFpIaInAJ0BqbnazMd6O6/3xH4XFWL7dGNMe4odlNfVb0i0g+YDcQBb6nqGhEZDixX1enAm8A7IrIB2I3vw8EYE6YC2sdX1ZnAzHzThuS5nwncEdxoxpiSEhFH7hljgssK35gYZIVvTAyywjcmBolb37qJSDrwSwBNKwG7SjjOyQjnfOGcDcI7Xzhng8DzXaiqlfNPdK3wAyUiy1U10e0chQnnfOGcDcI7Xzhng5PPZ5v6xsQgK3xjYlAkFP5YtwMUI5zzhXM2CO984ZwNTjJf2O/jG2OCLxJ6fGNMkFnhGxODwqbw3bygZxCyPSQia0VklYh8JiIXhipbIPnytOsgIioiIfuaKpBsItLJ//6tEZGJocoWSD4RuUBE5ovIt/6/780hzPaWiOwUkdWFzBcRedWffZWI1A144arq+g3f6b4bgYuAU4CVQJ18bR4A/u2/3xmYHEbZmgKn+e/fH6psgebztysHLAKWAInhkg2oCXwLnOV/fE44vXf4BtHu99+vA2wOYb5GQF1gdSHzbwZmAYLvUlVLA112uPT4rlzQM1jZVHW+qh72P1yC7ypFoRLIewcwAt/VjzPDLFsvYIyq7gFQ1Z1hlk+B3F9uKQ9sC1U4VV2E7/oWhWkPTFCfJUAFETkvkGWHS+EH7YKeLmXLqye+T+FQKTaffxOwmqrOCGEuCOy9qwXUEpHFIrJERFqFLF1g+YYBXUUkDd81KfqHJlpAjvf/5h8i9meyw5GIdAUSgcZuZ8klIqWAl4AeLkcpTGl8m/tN8G0pLRKRy1V1r5uh8kgC3lbVFBG5Dt+Vpi5T1Ry3g52McOnxw/mCnoFkQ0RaAE8B7VQ1KwS5chWXrxxwGbBARDbj2xecHqIBvkDeuzRguqo6qvoz8CO+D4JQCCRfT2AKgKp+BcTjO0EmHAT0f7NAoRqoKGYQozSwCajBn4Msl+Zr05ejB/emhFG2q/ENEtUMx/cuX/sFhG5wL5D3rhUw3n+/Er5N14phlG8W0MN/vza+fXwJ4d+3OoUP7rXh6MG9rwNebqheQAAv8GZ8n/Ybgaf804bj60HB90n7PrAB+Bq4KIyyzQN+A77z36aH03uXr23ICj/A907w7YqsBb4HOofTe4dvJH+x/0PhO+CmEGZ7D9gOOPi2jHoC9wH35Xnvxvizf388f1c7ZNeYGBQu+/jGmBCywjcmBlnhGxODrPCNiUFW+MbEICt8Y2KQFb4xMej/ATgX9D/fj9kDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the model with real data\n",
    "#train_for()\n",
    "train_while()\n",
    "lower_bound = tf.zeros(shape=[1,1]) # Lower bound u(0)=0\n",
    "upper_bound = tf.ones(shape=[1,1]) # Upper bound u(1)=1\n",
    "x = tf.sort(tf.random.uniform(shape=[1,98])) # Interior points\n",
    "test_points_tensor = tf.transpose(tf.concat([lower_bound, x, upper_bound], 1)) # Actual data\n",
    "test_points = test_points_tensor.numpy()\n",
    "g = model(test_points_tensor, training=False).numpy()\n",
    "x_actual = tf.sort(tf.random.uniform(shape=[1,1000]).numpy()) # Known solution\n",
    "#print(test_points[0])\n",
    "#print(x_actual[0])\n",
    "#print(f[0])\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title('1D Laplace (DVM)')\n",
    "ax.plot(x_actual[0],x_actual[0], label='u(x)', color='blue')\n",
    "ax.plot(test_points,g,'r--', label='g(x)')\n",
    "ax.legend()\n",
    "ax.set_aspect('equal','box')\n",
    "\n",
    "tikzplotlib.save(\"1d_laplace_dvm.tex\")\n",
    "\n",
    "average_error = np.sum(abs(g-test_points))/len(g)\n",
    "print(average_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "concerned-france",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.022790374755859374\n",
      "Epoch 000: Loss: 31259.350\n",
      "0.02458139181137085\n",
      "Epoch 001: Loss: 54.034\n",
      "0.017975629568099977\n",
      "Epoch 002: Loss: 53.777\n",
      "0.021562142372131346\n",
      "Epoch 003: Loss: 53.135\n",
      "0.01789018988609314\n",
      "Epoch 004: Loss: 52.227\n",
      "0.015678791999816893\n",
      "Epoch 005: Loss: 51.807\n",
      "0.010613255500793457\n",
      "Epoch 006: Loss: 51.342\n",
      "0.008882226943969727\n",
      "Epoch 007: Loss: 50.753\n",
      "0.006784302592277527\n",
      "Epoch 008: Loss: 50.371\n",
      "0.004778414368629456\n",
      "Epoch 009: Loss: 50.572\n",
      "0.020943682193756103\n",
      "Epoch 000: Loss: 1341.982\n",
      "0.022609233856201172\n",
      "Epoch 001: Loss: 50.796\n",
      "0.024339950084686278\n",
      "Epoch 002: Loss: 50.637\n",
      "0.026432318687438963\n",
      "Epoch 003: Loss: 50.545\n",
      "0.028537845611572264\n",
      "Epoch 004: Loss: 50.462\n",
      "0.025351135730743407\n",
      "Epoch 005: Loss: 50.289\n",
      "0.022583019733428956\n",
      "Epoch 006: Loss: 50.197\n",
      "0.022530758380889894\n",
      "Epoch 007: Loss: 50.156\n",
      "0.017663142681121825\n",
      "Epoch 008: Loss: 49.992\n",
      "0.014592018127441406\n",
      "Epoch 009: Loss: 49.889\n",
      "0.01270254135131836\n",
      "Epoch 010: Loss: 50.614\n",
      "0.01215829849243164\n",
      "Epoch 011: Loss: 49.964\n",
      "0.007519021034240723\n",
      "Epoch 012: Loss: 49.844\n",
      "0.006693345308303833\n",
      "Epoch 013: Loss: 49.832\n",
      "0.004926638007164002\n",
      "Epoch 014: Loss: 50.497\n",
      "0.025555939674377443\n",
      "Epoch 000: Loss: 992.072\n",
      "0.021942269802093507\n",
      "Epoch 001: Loss: 50.502\n",
      "0.021385397911071777\n",
      "Epoch 002: Loss: 49.961\n",
      "0.02046151876449585\n",
      "Epoch 003: Loss: 50.017\n",
      "0.013787921667099\n",
      "Epoch 004: Loss: 50.506\n",
      "0.0118057119846344\n",
      "Epoch 005: Loss: 50.081\n",
      "0.0073336011171340944\n",
      "Epoch 006: Loss: 50.374\n",
      "0.007114566564559936\n",
      "Epoch 007: Loss: 52.388\n",
      "0.005079498291015625\n",
      "Epoch 008: Loss: 50.122\n",
      "0.005171257257461548\n",
      "Epoch 009: Loss: 51.497\n",
      "0.00310645192861557\n",
      "Epoch 010: Loss: 52.900\n",
      "0.025962162017822265\n",
      "Epoch 000: Loss: 1326.107\n",
      "0.026969265937805176\n",
      "Epoch 001: Loss: 50.916\n",
      "0.027806549072265624\n",
      "Epoch 002: Loss: 50.892\n",
      "0.024606897830963134\n",
      "Epoch 003: Loss: 50.117\n",
      "0.0272171688079834\n",
      "Epoch 004: Loss: 50.182\n",
      "0.022653207778930665\n",
      "Epoch 005: Loss: 50.049\n",
      "0.020705609321594237\n",
      "Epoch 006: Loss: 50.049\n",
      "0.01631044626235962\n",
      "Epoch 007: Loss: 49.988\n",
      "0.010832867622375487\n",
      "Epoch 008: Loss: 49.980\n",
      "0.006908530592918396\n",
      "Epoch 009: Loss: 49.981\n",
      "0.004688348770141602\n",
      "Epoch 010: Loss: 49.944\n",
      "0.011992157697677612\n",
      "Epoch 000: Loss: 537.581\n",
      "0.010593128204345704\n",
      "Epoch 001: Loss: 49.689\n",
      "0.007170255184173584\n",
      "Epoch 002: Loss: 49.750\n",
      "0.004545166194438934\n",
      "Epoch 003: Loss: 50.333\n",
      "0.023832015991210938\n",
      "Epoch 000: Loss: 994.866\n",
      "0.023776140213012695\n",
      "Epoch 001: Loss: 50.210\n",
      "0.02619985580444336\n",
      "Epoch 002: Loss: 50.032\n",
      "0.02159630298614502\n",
      "Epoch 003: Loss: 50.021\n",
      "0.016092727184295653\n",
      "Epoch 004: Loss: 50.093\n",
      "0.010682072639465332\n",
      "Epoch 005: Loss: 49.923\n",
      "0.008308827877044678\n",
      "Epoch 006: Loss: 49.859\n",
      "0.007012112140655518\n",
      "Epoch 007: Loss: 49.928\n",
      "0.004303355515003204\n",
      "Epoch 008: Loss: 49.987\n",
      "0.024996626377105712\n",
      "Epoch 000: Loss: 920.654\n",
      "0.025510029792785646\n",
      "Epoch 001: Loss: 50.264\n",
      "0.021828250885009767\n",
      "Epoch 002: Loss: 49.935\n",
      "0.011909817457199096\n",
      "Epoch 003: Loss: 49.792\n",
      "0.008502326607704162\n",
      "Epoch 004: Loss: 50.278\n",
      "0.005548783540725708\n",
      "Epoch 005: Loss: 50.008\n",
      "0.004802960157394409\n",
      "Epoch 006: Loss: 49.860\n",
      "0.02297987222671509\n",
      "Epoch 000: Loss: 918.804\n",
      "0.02197209596633911\n",
      "Epoch 001: Loss: 50.688\n",
      "0.024959039688110352\n",
      "Epoch 002: Loss: 50.015\n",
      "0.01627555251121521\n",
      "Epoch 003: Loss: 49.870\n",
      "0.004304676055908203\n",
      "Epoch 004: Loss: 50.403\n",
      "0.019562215805053712\n",
      "Epoch 000: Loss: 940.553\n",
      "0.019795045852661133\n",
      "Epoch 001: Loss: 50.036\n",
      "0.017735017538070677\n",
      "Epoch 002: Loss: 49.916\n",
      "0.020077471733093263\n",
      "Epoch 003: Loss: 49.797\n",
      "0.014433221817016601\n",
      "Epoch 004: Loss: 50.045\n",
      "0.009523855447769165\n",
      "Epoch 005: Loss: 51.007\n",
      "0.008245945572853089\n",
      "Epoch 006: Loss: 49.971\n",
      "0.004491360783576965\n",
      "Epoch 007: Loss: 49.938\n",
      "0.02243861436843872\n",
      "Epoch 000: Loss: 388.708\n",
      "0.020090880393981932\n",
      "Epoch 001: Loss: 49.904\n",
      "0.00818994402885437\n",
      "Epoch 002: Loss: 49.746\n",
      "0.004307539761066436\n",
      "Epoch 003: Loss: 50.245\n",
      "0.0266147518157959\n",
      "Epoch 000: Loss: 907.958\n",
      "0.027308855056762695\n",
      "Epoch 001: Loss: 50.422\n",
      "0.027222070693969726\n",
      "Epoch 002: Loss: 50.204\n",
      "0.025466606616973878\n",
      "Epoch 003: Loss: 50.129\n",
      "0.016467888355255127\n",
      "Epoch 004: Loss: 50.154\n",
      "0.012797579765319825\n",
      "Epoch 005: Loss: 50.082\n",
      "0.004512365758419037\n",
      "Epoch 006: Loss: 50.095\n",
      "0.02412325382232666\n",
      "Epoch 000: Loss: 757.863\n",
      "0.022974474430084227\n",
      "Epoch 001: Loss: 50.743\n",
      "0.018715795278549194\n",
      "Epoch 002: Loss: 50.035\n",
      "0.0208013916015625\n",
      "Epoch 003: Loss: 49.921\n",
      "0.014890536069869995\n",
      "Epoch 004: Loss: 49.828\n",
      "0.010022276639938354\n",
      "Epoch 005: Loss: 50.023\n",
      "0.006906813979148865\n",
      "Epoch 006: Loss: 49.964\n",
      "0.002554771602153778\n",
      "Epoch 007: Loss: 50.091\n",
      "0.023849048614501954\n",
      "Epoch 000: Loss: 890.728\n",
      "0.024227161407470704\n",
      "Epoch 001: Loss: 50.668\n",
      "0.02044313907623291\n",
      "Epoch 002: Loss: 50.134\n",
      "0.017467234134674072\n",
      "Epoch 003: Loss: 49.849\n",
      "0.013187692165374756\n",
      "Epoch 004: Loss: 50.007\n",
      "0.00708710253238678\n",
      "Epoch 005: Loss: 49.967\n",
      "0.0004988089948892593\n",
      "Epoch 006: Loss: 50.147\n",
      "0.02640212535858154\n",
      "Epoch 000: Loss: 894.338\n",
      "0.027284669876098632\n",
      "Epoch 001: Loss: 50.672\n",
      "0.02561589241027832\n",
      "Epoch 002: Loss: 50.318\n",
      "0.026809868812561036\n",
      "Epoch 003: Loss: 50.047\n",
      "0.024142391681671142\n",
      "Epoch 004: Loss: 50.537\n",
      "0.022240056991577148\n",
      "Epoch 005: Loss: 50.118\n",
      "0.016770235300064086\n",
      "Epoch 006: Loss: 50.070\n",
      "0.01204763412475586\n",
      "Epoch 007: Loss: 50.006\n",
      "0.006920967698097229\n",
      "Epoch 008: Loss: 49.937\n",
      "0.0009998281300067902\n",
      "Epoch 009: Loss: 49.949\n",
      "0.026615257263183593\n",
      "Epoch 000: Loss: 849.684\n",
      "0.024299099445343017\n",
      "Epoch 001: Loss: 50.899\n",
      "0.03251340866088867\n",
      "Epoch 002: Loss: 50.494\n",
      "0.028658051490783692\n",
      "Epoch 003: Loss: 50.078\n",
      "0.029330148696899414\n",
      "Epoch 004: Loss: 50.294\n",
      "0.02286067008972168\n",
      "Epoch 005: Loss: 50.337\n",
      "0.020370469093322754\n",
      "Epoch 006: Loss: 50.126\n",
      "0.01384285807609558\n",
      "Epoch 007: Loss: 50.069\n",
      "0.0046220505237579345\n",
      "Epoch 008: Loss: 49.924\n",
      "0.02230588912963867\n",
      "Epoch 000: Loss: 658.578\n",
      "0.025131449699401856\n",
      "Epoch 001: Loss: 50.888\n",
      "0.023564281463623046\n",
      "Epoch 002: Loss: 50.277\n",
      "0.01980568766593933\n",
      "Epoch 003: Loss: 49.998\n",
      "0.0154638409614563\n",
      "Epoch 004: Loss: 49.998\n",
      "0.007684205174446106\n",
      "Epoch 005: Loss: 49.896\n",
      "0.0008715036511421204\n",
      "Epoch 006: Loss: 49.906\n",
      "0.027949585914611816\n",
      "Epoch 000: Loss: 771.553\n",
      "0.026192996501922607\n",
      "Epoch 001: Loss: 50.384\n",
      "0.015255610942840576\n",
      "Epoch 002: Loss: 49.939\n",
      "0.006286153793334961\n",
      "Epoch 003: Loss: 49.875\n",
      "0.0029049935936927796\n",
      "Epoch 004: Loss: 49.916\n",
      "0.016145362854003906\n",
      "Epoch 000: Loss: 565.505\n",
      "0.016319196224212646\n",
      "Epoch 001: Loss: 49.985\n",
      "0.014806327819824218\n",
      "Epoch 002: Loss: 49.740\n",
      "0.012118666172027589\n",
      "Epoch 003: Loss: 49.912\n",
      "0.005229208469390869\n",
      "Epoch 004: Loss: 49.880\n",
      "0.004262516498565674\n",
      "Epoch 005: Loss: 49.879\n",
      "0.01943850636482239\n",
      "Epoch 000: Loss: 221.602\n",
      "0.01198277473449707\n",
      "Epoch 001: Loss: 51.562\n",
      "0.004222323894500732\n",
      "Epoch 002: Loss: 50.477\n",
      "0.0246537184715271\n",
      "Epoch 000: Loss: 749.961\n",
      "0.026908392906188964\n",
      "Epoch 001: Loss: 50.264\n",
      "0.026956987380981446\n",
      "Epoch 002: Loss: 50.070\n",
      "0.022565677165985107\n",
      "Epoch 003: Loss: 50.117\n",
      "0.018492302894592284\n",
      "Epoch 004: Loss: 50.131\n",
      "0.01377810001373291\n",
      "Epoch 005: Loss: 50.077\n",
      "0.013138397932052612\n",
      "Epoch 006: Loss: 50.042\n",
      "0.006602665781974793\n",
      "Epoch 007: Loss: 49.949\n",
      "0.0011529567837715148\n",
      "Epoch 008: Loss: 49.947\n",
      "0.03242149353027344\n",
      "Epoch 000: Loss: 651.043\n",
      "0.03145697116851807\n",
      "Epoch 001: Loss: 50.841\n",
      "0.03698117733001709\n",
      "Epoch 002: Loss: 50.381\n",
      "0.02879302740097046\n",
      "Epoch 003: Loss: 50.149\n",
      "0.02852196216583252\n",
      "Epoch 004: Loss: 50.391\n",
      "0.022806777954101562\n",
      "Epoch 005: Loss: 50.197\n",
      "0.007805582284927368\n",
      "Epoch 006: Loss: 50.048\n",
      "0.0030494740605354308\n",
      "Epoch 007: Loss: 50.244\n",
      "0.034921822547912595\n",
      "Epoch 000: Loss: 708.003\n",
      "0.028886544704437255\n",
      "Epoch 001: Loss: 50.913\n",
      "0.03340652942657471\n",
      "Epoch 002: Loss: 50.204\n",
      "0.03245598793029785\n",
      "Epoch 003: Loss: 50.337\n",
      "0.02575475215911865\n",
      "Epoch 004: Loss: 50.158\n",
      "0.020128061771392824\n",
      "Epoch 005: Loss: 50.296\n",
      "0.012518792152404786\n",
      "Epoch 006: Loss: 50.229\n",
      "0.006104767322540283\n",
      "Epoch 007: Loss: 49.875\n",
      "0.0020436075329780578\n",
      "Epoch 008: Loss: 49.906\n",
      "0.020295534133911133\n",
      "Epoch 000: Loss: 685.565\n",
      "0.01839837908744812\n",
      "Epoch 001: Loss: 50.955\n",
      "0.019862202405929567\n",
      "Epoch 002: Loss: 50.379\n",
      "0.02014662504196167\n",
      "Epoch 003: Loss: 49.906\n",
      "0.016041227579116822\n",
      "Epoch 004: Loss: 49.811\n",
      "0.01115111231803894\n",
      "Epoch 005: Loss: 49.950\n",
      "0.004362525939941406\n",
      "Epoch 006: Loss: 49.990\n",
      "0.008006865978240968\n",
      "Epoch 000: Loss: 243.016\n",
      "0.0026832866668701173\n",
      "Epoch 001: Loss: 49.672\n",
      "0.030555381774902343\n",
      "Epoch 000: Loss: 649.375\n",
      "0.03200646162033081\n",
      "Epoch 001: Loss: 50.665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.030855963230133055\n",
      "Epoch 002: Loss: 50.388\n",
      "0.0292840576171875\n",
      "Epoch 003: Loss: 50.252\n",
      "0.025808916091918946\n",
      "Epoch 004: Loss: 50.274\n",
      "0.016314547061920166\n",
      "Epoch 005: Loss: 50.095\n",
      "0.004882034659385681\n",
      "Epoch 006: Loss: 49.983\n",
      "0.025812792778015136\n",
      "Epoch 000: Loss: 491.749\n",
      "0.024115691184997557\n",
      "Epoch 001: Loss: 50.857\n",
      "0.020028209686279295\n",
      "Epoch 002: Loss: 50.089\n",
      "0.01493816614151001\n",
      "Epoch 003: Loss: 49.936\n",
      "0.007416985034942627\n",
      "Epoch 004: Loss: 49.884\n",
      "0.0016963553428649902\n",
      "Epoch 005: Loss: 49.949\n",
      "0.02575261116027832\n",
      "Epoch 000: Loss: 617.766\n",
      "0.02708092451095581\n",
      "Epoch 001: Loss: 51.338\n",
      "0.024208083152770996\n",
      "Epoch 002: Loss: 50.504\n",
      "0.020903346538543702\n",
      "Epoch 003: Loss: 49.972\n",
      "0.01216444730758667\n",
      "Epoch 004: Loss: 49.934\n",
      "0.001988593637943268\n",
      "Epoch 005: Loss: 50.025\n",
      "0.020441827774047853\n",
      "Epoch 000: Loss: 624.345\n",
      "0.019236342906951906\n",
      "Epoch 001: Loss: 50.889\n",
      "0.013284401893615723\n",
      "Epoch 002: Loss: 50.097\n",
      "0.004673861265182495\n",
      "Epoch 003: Loss: 49.699\n",
      "0.03610732793807983\n",
      "Epoch 000: Loss: 667.445\n",
      "0.026731958389282228\n",
      "Epoch 001: Loss: 50.705\n",
      "0.0245589017868042\n",
      "Epoch 002: Loss: 49.998\n",
      "0.01209130048751831\n",
      "Epoch 003: Loss: 49.816\n",
      "0.006070080995559692\n",
      "Epoch 004: Loss: 49.882\n",
      "0.0016303740441799164\n",
      "Epoch 005: Loss: 50.203\n",
      "0.034932551383972166\n",
      "Epoch 000: Loss: 642.154\n",
      "0.036285266876220704\n",
      "Epoch 001: Loss: 50.879\n",
      "0.033648386001586914\n",
      "Epoch 002: Loss: 50.438\n",
      "0.034651706218719484\n",
      "Epoch 003: Loss: 50.513\n",
      "0.025086755752563476\n",
      "Epoch 004: Loss: 50.334\n",
      "0.020338258743286132\n",
      "Epoch 005: Loss: 50.344\n",
      "0.0057703226804733275\n",
      "Epoch 006: Loss: 50.088\n",
      "7.458925247192383e-05\n",
      "Epoch 007: Loss: 50.097\n",
      "428.7089973129987\n"
     ]
    }
   ],
   "source": [
    "print(timeit.timeit(train_while, number=30))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
